{
 "metadata": {
  "name": "week12-02-multilayer-perceptron"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The Multi-Layer Perceptron"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Perceptron is a linear model\n",
      "* Why?\n",
      "* Inherently simple - two solutions. \n",
      "  - Transform the features to make the problem linearly separable\n",
      "  - Make the network more complex\n",
      "* The **multi-layer perceptron** builds on the second idea"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Learning with the perceptron occurs in the weights\n",
      "* To make a more complex network, add more weights\n",
      "* Two ways to do so\n",
      "  - Add some backward connections - so neurons connect to inputs again\n",
      "  - Add more neurons\n",
      "* The first approach leads to **recurrent networks**\n",
      "* The second approach leads to MLP\n",
      "* It will allow us to add more \"layers\" of nuerons between the input nodes and the outputs"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Show that we can solve the XOR Problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Worked in class"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Going Forward"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Going *forward* means working out the outputs for given inputs\n",
      "* This is the *recall* phase we discussed last time\n",
      "* This is the same in the MLP as the Perceptron\n",
      "* Only we do it twice now - layer-by-layer\n",
      "* The activations of one layer of nodes are the inputs to the next"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Going Backwards: Back-Propagation of Error"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Computing the errors is the same\n",
      "* What to do with them - how to update the weights - is more difficult\n",
      "* This is called **back-propagation of error**\n",
      "* We do so through **gradient descent**\n",
      "* We know we need to update the weights, but which ones? Inputs $\\rightarrow$ hidden layer or hidden layer $\\rightarrow$ outputs?\n",
      "* Recall that our error function in the Perceptron was $E=t-y$\n",
      "* For the MLP we will use the familiar sum of squares error $E(t,y)=\\frac{1}{2}\\sum_{k=1}^n(t_k-y_k)^2$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Activation Function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We also need to revisit our activation function\n",
      "* For the Perceptron, we used a binary activation function\n",
      "* This is not differentiable\n",
      "* For the MLP, we will use a *sigmoid* function\n",
      "* You have seen an exmaple in the logistic function with the now-familiar S-shape\n",
      "$$g(h)=\\frac{1}{1+e^{-\\beta h}}$$\n",
      "\n",
      "where $\\beta > 0$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots()\n",
      "g_func = lambda h, beta : 1/(1 + np.exp(-beta*h))\n",
      "x = np.linspace(-3, 3, 100)\n",
      "\n",
      "ax.plot(x, g_func(x, 2))\n",
      "ax.hlines([0, 1], -3, 3, color=\"r\")\n",
      "ax.set_ylim(-.1, 1.1);\n",
      "ax.set_title(r\"$g(h;\\, \\beta)$\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Back-Propagation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We feed our inputs forward through the network, which tells us which nodes are firing\n",
      "* We compute the errors\n",
      "* We need to compute the gradient of the errors with respect to the weights\n",
      "* This allows us to update the weights in the \"downhill\" direction\n",
      "  * Do this for the nodes connected to the output layer\n",
      "  * Work backwards until we get back to the inputs again\n",
      "* Two problems\n",
      "  * We don't know the inputs to the output neurons\n",
      "  * We don't know the targets for the hidden neurons (for multiple hidden layers - don't know the inputs or the outputs...)\n",
      "* We can use the chain-rule from calculus to get around this"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "MLP Algorithm Overview"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "* An input vector is put into the input nodes\n",
      "* Inputs are fed *forward* through the network\n",
      "  * Inputs and first-layer weights $v$ determine wither the hidden nodes fire via $g(\\cdot)$\n",
      "  * Outputs of these nodes and the second-layer weights are used to decide if the output neurons fire\n",
      "* Sum-of-squares error is computed\n",
      "* Error is fed backwards through the network\n",
      "  * Second-layer weights are updated (using $\\delta_0$)\n",
      "  * First-layer weights are updated (using $\\delta_h$)\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "MLP Algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Initialization of weights\n",
      "* **Training** (Repeat)\n",
      "  * For each input vector\n",
      "    \n",
      "    **Forwards phase**\n",
      "    \n",
      "     * Compute the activation of each neuron $j$ in the hidden layer(s) using\n",
      "     <br /><br />\n",
      "    \n",
      "    $$h_j=\\sum_ix_iv_{ij}$$\n",
      "    $$a_j=g(h_j)=\\frac{1}{1+e^{-\\beta h_j}}$$\n",
      "    \n",
      "    <br />\n",
      "    \n",
      "     * Work through network until you get the output layers\n",
      "        \n",
      "    <br />\n",
      "    \n",
      "    $$h_k=\\sum_j{a_jw_{jk}}$$\n",
      "    $$y_k=g(h_k)=\\frac{1}{1+e^{-\\beta h_k}}$$\n",
      "    \n",
      "    **Backwards phase**\n",
      "    \n",
      "     * compute the error at the output using\n",
      "        \n",
      "        $$\\delta_{ok}=(t_k-y_k)y_k(1-y_k)$$\n",
      "     \n",
      "     * compute the error in the hidden layer(s) using\n",
      "        \n",
      "        $$\\delta_{hj}=a_j(1-a_j)\\sum_k w_{jk}\\delta_{ok}$$\n",
      "      \n",
      "     * update the output layer weights using\n",
      "        \n",
      "        $$w_{jk}\\leftarrow w_{jk} + \\eta \\delta_{ok}a_j$$\n",
      "        \n",
      "     * update the hidden layer weights using\n",
      "        \n",
      "        $$v_{ij}\\leftarrow v_{ij}+\\eta \\delta_{hj}x_i$$\n",
      "<br/>\n",
      "\n",
      "    Randomize the order of the input vectors so that you don't train in exactly the same order each iteration\n",
      "    \n",
      "    \n",
      "* **Recall**\n",
      "  * Use the Forwards Phase"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Implementation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Batch vs. Online algorithms**\n",
      "\n",
      "* The implementation below is what's called a **batch** algorithm\n",
      "* This means that the weights are only update after all the training inputs have been seen\n",
      "* The weights are updated once for each **epoch** (pass through training examples)\n",
      "* The gradient estimation will be more accurate and will thus converge to the local minimum more quickly"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The algorithm described above is an **online** algorithm\n",
      "* It is **sequential**\n",
      "* An online algorithm would update the weights incrementally over the training inputs\n",
      "* Online algorithms have a few advantages \n",
      "  * They can be more efficient in terms of memory use\n",
      "  * They can stop \"early\"\n",
      "  * They can avoid local minimum by using less accurate gradients\n",
      "* Online algorithms are not always (easily) available"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Initial weights**\n",
      "\n",
      "* Each neuron gets an input from $n$ different places (inputs nodes or hidden neurons)\n",
      "* If they all have the same variance, then the typical input for each neuron is $w\\sqrt{n}$\n",
      "* A common method is then to set the weights so that $\\frac{-1}{n}<w<\\frac{1}{n}$\n",
      "* This is called **uniform learning**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Using different activation functions**\n",
      "\n",
      "* Regression problems: linear activation\n",
      "\n",
      "$$y_k = g(h_k)=h_k$$\n",
      "\n",
      "* This needs a new delta term for the update step\n",
      "\n",
      "$$\\delta_{ok}=(t_k-y_k)$$\n",
      "    \n",
      "* $1\\mbox{-of-}N$ output encoding: soft-max\n",
      "* $1\\mbox{-of-}N$ output encoding is used when the output variable can take on more than two values\n",
      "* For example, modeling the decision of transportation, \"bus\", \"car\", or \"bike\" might be encoded\n",
      "\n",
      "&nbsp;\n",
      "\n",
      "    [[1, 0, 0],\n",
      "     [0, 1, 0],\n",
      "     [0, 0, 1]]\n",
      "\n",
      "* The soft-max function is the same function that appears in Multinomial Logit problems in statistics\n",
      "\n",
      "$$y_k = g(h_k) = \\frac{e^{h_k}}{\\sum_{h_k}e^{h_k}}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Local Minima**\n",
      "\n",
      "* As we saw when discussing optimization, local minima can be a problem \n",
      "* This is also the case for gradient descent\n",
      "* The problem is exacerbated by the higher dimensionality\n",
      "* One way to try to overcome getting stuck in local minima is by picking up **momentum**\n",
      "* Momentum also allows us to use a smaller (and thus more stable) learning rate $\\eta$\n",
      "* We add momentum to the weights updating as follows\n",
      "\n",
      "$$w_{ij}^t\\leftarrow w_{ij}^{t-1}+\\eta \\delta_0 a_j + \\alpha \\Delta w_{ij}^{t-1}$$\n",
      "\n",
      "where $0 < \\alpha < 1$ is the momentum constant"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.set_printoptions(suppress=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from perceptron import Perceptron, add_bias_node"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Functions for internal use to avoid [spaghetti code](http://en.wikipedia.org/wiki/Spaghetti_code)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def _linear_delta(targets, outputs, nobs):\n",
      "    return (targets - outputs)/nobs\n",
      "\n",
      "def _logistic_delta(targets, outputs, *args):\n",
      "    return (targets - outputs)*outputs\n",
      "\n",
      "def _softmax_delta(targets, outputs, nobs):\n",
      "    return (targets - outputs)/nobs\n",
      "\n",
      "_calc_deltao = {\n",
      "        \"linear\" : _linear_delta,\n",
      "        \"logistic\" : _logistic_delta,\n",
      "        \"softmax\" : _softmax_delta\n",
      "        }\n",
      "\n",
      "def _linear_activation(outputs, *args):\n",
      "    return outputs\n",
      "\n",
      "def _logistic_activation(outputs, beta, *args):\n",
      "    return 1/(1+np.exp(-beta*outputs))\n",
      "\n",
      "def _softmax_activation(outputs, *args):\n",
      "    # this is multinomial logit\n",
      "    eX = np.exp(outputs)\n",
      "    return eX/eX.sum(axis=1)[:,None]\n",
      "\n",
      "\n",
      "_activation_funcs = {\n",
      "        \"linear\" : _linear_activation,\n",
      "        \"logistic\" : _logistic_activation,\n",
      "        \"softmax\" : _softmax_activation,\n",
      "        }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from perceptron import Perceptron, add_bias_node\n",
      "\n",
      "class MLP(Perceptron):\n",
      "    \"\"\"\n",
      "    A Multi-Layer Perceptron\n",
      "    \"\"\"\n",
      "    def __init__(self, nhidden, eta, beta=1, momentum=0.9, outtype='logistic'):\n",
      "        # Set up network size\n",
      "        self.nhidden = nhidden\n",
      "        self.eta = eta\n",
      "\n",
      "        self.beta = beta\n",
      "        self.momentum = momentum\n",
      "        self.outtype = outtype\n",
      "\n",
      "\n",
      "    def _init_weights(self):\n",
      "        # Initialise network\n",
      "        weights1 = np.random.rand(self.m+1, self.nhidden)-0.5\n",
      "        weights1 *= 2/np.sqrt(self.m)\n",
      "        weights2 = np.random.rand(self.nhidden+1,self.n)-0.5\n",
      "        weights2 *= 2/np.sqrt(self.nhidden)\n",
      "\n",
      "        self.weights1 = weights1\n",
      "        self.weights2 = weights2\n",
      "\n",
      "    def earlystopping(self, inputs, targets, valid_input, valid_target,\n",
      "                            max_iter=100, epsilon=1e-3, disp=True):\n",
      "\n",
      "        self._initialize(inputs, targets)\n",
      "        valid_input = add_bias_node(valid_input)\n",
      "\n",
      "        # 2 iterations ago, last iteration, current iteration\n",
      "        # current iteration, last iteration, 2 iterations ago\n",
      "        last_errors = [0, np.inf, np.inf]\n",
      "\n",
      "        count = 0\n",
      "\n",
      "        while np.any(np.diff(last_errors) > epsilon):\n",
      "            count += 1\n",
      "            if disp:\n",
      "                print count\n",
      "\n",
      "            # train the network\n",
      "            self.fit(inputs, targets, max_iter, init=False, disp=disp)\n",
      "            last_errors[2] = last_errors[1]\n",
      "            last_errors[1] = last_errors[0]\n",
      "\n",
      "            # check on the validation set\n",
      "            valid_output = self.predict(valid_input, add_bias=False)\n",
      "            errors = valid_target - valid_output\n",
      "            last_errors[0] = 0.5*np.sum(errors**2)\n",
      "        \n",
      "        if disp:\n",
      "            print \"Stopped in %d iterations\" % count, last_errors\n",
      "        return last_errors[0]\n",
      "\n",
      "    def fit(self, inputs, targets, max_iter, disp=True, init=True):\n",
      "        \"\"\"\n",
      "        Train the network\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        inputs : array-like\n",
      "            The inputs data\n",
      "        targets : array-like\n",
      "            The targets to train on\n",
      "        max_iter : int\n",
      "            The number of iterations to perform\n",
      "        init : bool\n",
      "            Whether to initialize the weights or not.\n",
      "        \"\"\"\n",
      "        if init:\n",
      "            self._initialize(inputs, targets)\n",
      "        inputs = self.inputs\n",
      "        targets = self.targets\n",
      "        weights1 = self.weights1\n",
      "        weights2 = self.weights2\n",
      "        eta = self.eta\n",
      "        momentum = self.momentum\n",
      "        nobs = self.nobs\n",
      "\n",
      "        outtype = self.outtype\n",
      "\n",
      "        # Add the inputs that match the bias node\n",
      "        inputs = add_bias_node(inputs)\n",
      "        change = range(self.nobs)\n",
      "\n",
      "        updatew1 = np.zeros_like(weights1)\n",
      "        updatew2 = np.zeros_like(weights2)\n",
      "\n",
      "\n",
      "        for n in range(1, max_iter+1):\n",
      "\n",
      "            # predict attaches hidden\n",
      "            outputs = self.predict(inputs, add_bias=False)\n",
      "\n",
      "            error = targets - outputs\n",
      "            obj = .5 * np.sum(error**2)\n",
      "\n",
      "            # Different types of output neurons\n",
      "            deltao = _calc_deltao[outtype](targets, outputs, nobs)\n",
      "            hidden = self.hidden\n",
      "            deltah = hidden * (1. - hidden) * np.dot(deltao, weights2.T)\n",
      "\n",
      "            updatew1 = (eta*(np.dot(inputs.T, deltah[:,:-1])) +\n",
      "                        momentum*updatew1)\n",
      "            updatew2 = (eta*(np.dot(self.hidden.T, deltao)) +\n",
      "                        momentum*updatew2)\n",
      "            weights1 += updatew1\n",
      "            weights2 += updatew2\n",
      "\n",
      "            # Randomise order of inputs\n",
      "            np.random.shuffle(change)\n",
      "            inputs = inputs[change,:]\n",
      "            targets = targets[change,:]\n",
      "\n",
      "        if disp:\n",
      "            print \"Iteration: \", n, \" Objective: \", obj\n",
      "\n",
      "        # attach results\n",
      "        self.weights1 = weights1\n",
      "        self.weights2 = weights2\n",
      "        self.outputs = outputs\n",
      "\n",
      "    def predict(self, inputs=None, add_bias=True):\n",
      "        \"\"\"\n",
      "        Run the network forward.\n",
      "        \"\"\"\n",
      "        if inputs is None:\n",
      "            inputs = self.inputs\n",
      "\n",
      "        if add_bias:\n",
      "            inputs = add_bias_node(inputs)\n",
      "\n",
      "        hidden = np.dot(inputs, self.weights1)\n",
      "        hidden = _activation_funcs[\"logistic\"](hidden, self.beta)\n",
      "        hidden = add_bias_node(hidden)\n",
      "        self.hidden = hidden\n",
      "\n",
      "        outputs = np.dot(self.hidden, self.weights2)\n",
      "\n",
      "        outtype = self.outtype\n",
      "\n",
      "        # Different types of output neurons\n",
      "        return _activation_funcs[self.outtype](outputs, self.beta)\n",
      "\n",
      "    def confusion_matrix(self, inputs, targets, summary=True):\n",
      "        \"\"\"\n",
      "        Confusion matrix.\n",
      "        \"\"\"\n",
      "        targets = np.asarray(targets)\n",
      "\n",
      "        # Add the inputs that match the bias node\n",
      "        inputs = add_bias_node(inputs)\n",
      "        # predict attaches hidden\n",
      "        outputs = self.predict(inputs, add_bias=False)\n",
      "\n",
      "        n_classes = targets.ndim == 1 and 1 or targets.shape[1]\n",
      "\n",
      "        if n_classes==1:\n",
      "            n_classes = 2\n",
      "            # 50% cut-off with continuous activation function\n",
      "            outputs = np.where(outputs > 0.5, 1, 0)\n",
      "        else:\n",
      "            # 1-of-N encoding\n",
      "            outputs = np.argmax(outputs, 1)\n",
      "            targets = np.argmax(targets, 1)\n",
      "\n",
      "        outputs = np.squeeze(outputs)\n",
      "        targets = np.squeeze(targets)\n",
      "\n",
      "        cm = np.histogram2d(targets, outputs, bins=n_classes)[0]\n",
      "\n",
      "        if not summary:\n",
      "            return cm\n",
      "        else:\n",
      "            return np.trace(cm)/np.sum(cm)*100"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "AND Example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = [[0., 0.],\n",
      "     [0., 1.],\n",
      "     [1., 0.],\n",
      "     [1., 1.]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target = [0, 0, 0, 1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "and_clf = MLP(2, .25)\n",
      "and_clf.fit(X, target, 5001)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "and_clf.predict(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "and_clf.confusion_matrix(X, target, summary=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "XOR Example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target = [0., 1., 1., 0.]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xor_clf = MLP(2, .25)\n",
      "xor_clf.fit(X, target, 5001)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xor_clf.predict()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Practical Considerations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Data Preparation\n",
      "* How much training data?\n",
      "* Number of hidden layers?\n",
      "* Overfitting\n",
      "   * Stop before overfitting\n",
      "   * How do we figure this out?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Training, Testing, and Validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* To evaluate our training, we may use the **holdout method**\n",
      "* We set aside some data from the training training set\n",
      "* This data is called the **test set**\n",
      "* It reduces the data available for training\n",
      "* We also want to try to evaluate how well the learning is going during training\n",
      "* For this we use a **validation set** (the same idea as **cross-validation** in statistics)\n",
      "* We might split the data into training:test:validation according to 50:25:25 or 60:20:20\n",
      "* This is called a **three-way data split**\n",
      "* You may need to be sure that the data is randomly ordered before splitting\n",
      "* All data preprocessing occurs before splitting"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Other Resampling Methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* It is common to have too little data for a proper training-testing-validation split\n",
      "* We might also want to avoid getting a small error rate because of \"bad\" random split\n",
      "* At the expense of more computations, we may try cross-validation\n",
      "  * Random Subsampling\n",
      "  * K-Fold Cross-Validation\n",
      "  * Leave-one-out Cross Validation\n",
      "* The idea is to randomly partition the dataset into $K$ subsets\n",
      "* Use one of the $K$ subsets as a validation set and train on the others\n",
      "* Do this again, leaving out another subset for validation, until all are left-out for validation\n",
      "* Use the model with the smallest validation error"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "When to Stop?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* So far we have trained a network for a fixed number of iterations\n",
      "* This is never what you really want to do\n",
      "* You run the potential for both over- and under-fitting the data.\n",
      "* We can use the **validation set** to determine when to stop\n",
      "* Run the network for some fixed number of iterations and then evaluate on the validation set\n",
      "* Run for a few more iterations, then start over\n",
      "* At some point the error on the validation set will start to increase\n",
      "* This is when we stop. Unsurprisingly, this is called **early stopping**"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Example: Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* First let's generate some data\n",
      "* Then we'll use the MLP to try to uncover the function (or data generating process)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(12345)\n",
      "x = np.linspace(0, 1, 40)[:,None] # make 2D\n",
      "t = (np.sin(2*np.pi*x) + \n",
      "     np.cos(4*np.pi*x) + \n",
      "     np.random.normal(0, .2, size=(40,1)))\n",
      "x = (x - .5)*2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots()\n",
      "ax.plot(x, t, 'o');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Split into 50:25:25"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train = x[::2]\n",
      "test = x[1::4]\n",
      "validate = x[3::4]\n",
      "\n",
      "train_target = t[::2]\n",
      "test_target = t[1::4]\n",
      "validate_target = t[3::4]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We don't know how many hidden neurons, we'll need, so let's try 3.\n",
      "* We will run this for 100 iterations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net = MLP(3, .25, outtype=\"linear\")\n",
      "net.fit(train, train_target, 100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* First let's decide how long to train the network"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net.earlystopping(train, train_target, validate, validate_target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Now we need to figure out how to select the number of nodes we want\n",
      "* To find out, we can run each network size a number of times, say 50 and keep the error"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nhiddens = [1, 2, 3, 5, 10, 25, 50, 100]\n",
      "all_errors = []\n",
      "nruns = 10\n",
      "\n",
      "for nhidden in nhiddens:\n",
      "    errors = []\n",
      "    mlp = MLP(nhidden, .25, outtype=\"linear\")\n",
      "    for i in range(nruns):\n",
      "        error = mlp.earlystopping(train, train_target, validate, validate_target, disp=False)\n",
      "        errors.append(error)\n",
      "    all_errors.append(errors)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_errors = np.array(all_errors)\n",
      "print \"        n              mean          std            min            max\"\n",
      "print np.column_stack((nhiddens, all_errors.mean(1), all_errors.std(1),\n",
      "                       all_errors.min(1), all_errors.max(1)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "scikit-learn for training, testing, and cross-validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Documentation](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cross_validation)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation\n",
      "\n",
      "a, b = np.arange(16).reshape((8, 2)), np.arange(8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print a\n",
      "print b"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(a_train, \n",
      " a_test, \n",
      " b_train, \n",
      " b_test) = cross_validation.train_test_split(a, b, \n",
      "                                     train_size=.75,\n",
      "                                     random_state=123)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a_train, b_train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a_test, b_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Aside:** Python Generators"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**K-Folds Cross-Validation**\n",
      "\n",
      "* The data is split into $K$ consecutive \"folds\"\n",
      "* Of the K subsamples, retain a single subsample as a validation set\n",
      "* Use the other k-1 subsamples as a training set\n",
      "* Gives indices to split the data into train and test sets\n",
      "* All observations are used as both training and test samples"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "y = np.array([1, 2, 3, 4])\n",
      "kf = cross_validation.KFold(10, n_folds=5)\n",
      "\n",
      "for train_index, test_index in kf:\n",
      "    print \"TRAIN:\", train_index, \"TEST:\", test_index"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kf = cross_validation.KFold(10, n_folds=5, shuffle=True)\n",
      "\n",
      "for train_index, test_index in kf:\n",
      "    print \"TRAIN:\", train_index, \"TEST:\", test_index"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Example: Classification"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_iris\n",
      "\n",
      "data = load_iris()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print data.DESCR"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can read much more about the dataset [here](http://en.wikipedia.org/wiki/Iris_flower_data_set)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Normalize the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# demean\n",
      "X = data.data - data.data.mean(0)\n",
      "# normalize by maximum\n",
      "X /= X.max(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need to put this into 1-of-N encoding"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target = np.zeros((len(X), 3))\n",
      "target[np.arange(len(X)),data.target] = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are going to use train_test_split to split the data up into training, testing, and validation sets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(feature_train, feature_test,\n",
      " target_train, target_test) = cross_validation.train_test_split(X, target, test_size=.25)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target_train.mean(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target_validate.mean(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you need to split based on maintaining the same percentage of the target labels, you can use [StratifiedKFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedKFold.html#sklearn.cross_validation.StratifiedKFold)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now split the training data into training and validation sets "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(feature_train, feature_validate,\n",
      " target_train, target_validate) = cross_validation.train_test_split(feature_train, \n",
      "                                                                    target_train, \n",
      "                                                                    test_size=.33)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net = MLP(5, .1, outtype=\"softmax\")\n",
      "net.earlystopping(feature_train, target_train, feature_validate, \n",
      "                  target_validate, disp=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net.confusion_matrix(feature_test, target_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net.confusion_matrix(feature_test, target_test, summary=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Back-Propagation Derivation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The output, $y$, is a function of $x$, $g(\\cdot)$, and the weights\n",
      "* The weights will be denoted $v$ and $w$ for the first and second layers\n",
      "* $i$ is the index over the input nodes, $j$ is the index over the hidden layer neurons, $k$ is the index over the output neurons\n",
      "* First let's write the error function\n",
      "\n",
      "$$\\begin{aligned}E(\\boldsymbol{w}) & =\\frac{1}{2}\\sum_{k=1}^N\\left(t_k-y_k\\right)^2 \\cr\n",
      "& = \\frac{1}{2}\\sum_k{\\left[t_k-g\\left(\\sum_j w_{jk}a_j \\right)\\right]^2}\\end{aligned}$$\n",
      "\n",
      "where $a_j$ is the output from the hidden layer neurons\n",
      "\n",
      "* For the moment, let's ignore the hidden layer and work with the perceptron\n",
      "\n",
      "$$\\begin{aligned}E(\\boldsymbol{w}) & =\\frac{1}{2}\\sum_{k=1}^N\\left(t_k-y_k\\right)^2 \\cr\n",
      "& = \\frac{1}{2}\\sum_k{\\left[t_k-g\\left(\\sum_j w_{jk}x_j \\right)\\right]^2}\\end{aligned}$$\n",
      "\n",
      "* Since we will use gradient descent, unsurprisingly, we will need the gradient\n",
      "* Let's remind ourselves, what is the gradient again?\n",
      "* Recall that $g$ was the binary activation function and, thus, not differentiable, so we ignore it in the below\n",
      "* We adjust the weights to reduce the errors, thus we need\n",
      "\n",
      "\n",
      "$$\\begin{aligned}\\frac{\\partial E}{\\partial w_{ik}} & = \\frac{\\partial}{\\partial w_{ik}}\\left(\\frac{1}{2}\\sum_k\\left(t_k-\\sum_j w_{jk}x_j\\right)^2\\right) \\cr\n",
      "& = \\frac{1}{2}\\sum_k2(t_k-y_k)\\frac{\\partial}{\\partial w_{ik}}\\left(t_k-\\sum_j w_{jk}x_j\\right) \\cr\n",
      "\\end{aligned}\n",
      "$$\n",
      "\n",
      "note that\n",
      "\n",
      "$$\\frac{\\partial t_k}{\\partial w_{ik}}=0$$\n",
      "\n",
      "and \n",
      "\n",
      "$\\frac{\\partial}{\\partial w_{ik}}\\sum_jw_{jk}x_j$\n",
      "\n",
      "is only non-zero when $i=j$, thus\n",
      "\n",
      "$$\\frac{\\partial}{\\partial w_{ik}}=x_i$$\n",
      "\n",
      "so that we have\n",
      "\n",
      "$$\\begin{aligned}\\frac{\\partial E}{\\partial w_{ik}} & = \\sum_k(t_k-y_k)\\left(-x_i\\right)\n",
      "\\end{aligned}\n",
      "$$\n",
      "\n",
      "To make our errors smaller, we follow the gradient \"downhill\" such that (including the learning rate)\n",
      "\n",
      "$$w_{ik}\\leftarrow w_{ik}+\\eta(t_k-y_k)x_i$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Working with the Activation Function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* From the above, it is clear that we need a differentiable $g$ to obtain $\\frac{\\partial g}{\\partial w_{ik}}$\n",
      "$$a = g(h)=\\frac{1}{1+e^{-\\beta h}}$$\n",
      "* The derivative of $g$ has a simple form\n",
      "$$\\begin{aligned}\n",
      "g^{\\prime}(h) & = \\frac{d}{dh}\\frac{1}{1+e^{-\\beta h}} \\cr\n",
      "& = \\frac{d}{dh}(1+e^{-\\beta h})^{-1} \\cr\n",
      "& = -(1+e^{-\\beta h})^{-2}(-\\beta e^{-\\beta h}) \\cr\n",
      "& = \\frac{\\beta e^{-\\beta h}}{(1+e^{-\\beta h})^{2}} \\cr\n",
      "& = \\beta\\frac{1}{1+e^{-\\beta h}}\\frac{ e^{-\\beta h}}{1+e^{-\\beta h}} \\cr\n",
      "& = \\beta\\frac{1}{1+e^{-\\beta h}}\\left(\\frac{1 + e^{-\\beta h} - 1}{1+e^{-\\beta h}}\\right) \\cr\n",
      "& = \\beta g(h)(1-g(h)) \\cr\n",
      "& = \\beta a(1-a) \\cr\n",
      "\\end{aligned}\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Back-Propagation of Error"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* To use gradient-descent in the MLP, we need the partials of the errors with respect to each weight\n",
      "\n",
      "$$\\frac{\\partial E}{\\partial w_{jk}}=\\frac{\\partial E}{\\partial h_k}\\frac{\\partial h_k}{\\partial w_{jk}}$$\n",
      "\n",
      "where $h_k=\\sum_lw_{lk}a_l$ is the input to output-layer neuron $k$\n",
      "* Ie., this is the weighted average of the activations of the hidden layer neurons, using second-layer weights\n",
      "* Taking the second part of partials, we have\n",
      "\n",
      "$$\\begin{aligned}\n",
      "{\\partial h_k}\\frac{\\partial h_k}{\\partial w_{jk}} & =\\frac{\\partial \\sum_l w_{lk}a_l}{\\partial w_{jk}} \\cr\n",
      "& = \\sum_l\\frac{\\partial w_{lk}a_l}{\\partial w_{jk}} \\cr\n",
      "& = a_j\n",
      "\\end{aligned}\n",
      "$$\n",
      "\n",
      "* The first term is referred to as the **error** or **delta term**\n",
      "\n",
      "$$\\delta_0 = \\frac{\\partial E}{\\partial h_k}$$\n",
      "\n",
      "* We need to unpack this, because we don't know the inputs just the outputs\n",
      "\n",
      "$$\\delta_0 = \\frac{\\partial E}{\\partial h_k}= \\frac{\\partial E}{\\partial y_k}\\frac{\\partial y_k}{\\partial h_k}$$\n",
      "\n",
      "where the output of the output-layer neuron $k$\n",
      "\n",
      "$$y_k = g(h_k)=g\\left(\\sum_j w_{jk}a_j\\right)$$\n",
      "\n",
      "Plugging-in the derivatives we already have for $\\delta_0$ gives\n",
      "\n",
      "$$\\begin{aligned}\n",
      "\\delta_0 & = \\frac{\\partial E}{\\partial g(h_k)}\\frac{\\partial g(h_k)}{\\partial h_k} \\cr\n",
      "& = \\frac{\\partial}{\\partial g(h_k)}\\left[\\frac{1}{2}\\sum_k{\\left(t_k-g\\left(\\sum_j w_{jk}x_j\\right)\\right)}\\right] \\frac{\\partial g(h_k)}{\\partial h_k} \\cr\n",
      "& = (g(h_k)-t_k)g^{\\prime}(h_k) \\cr\n",
      "& = (y_k - t_k)g^{\\prime}(h_k)\n",
      "\\end{aligned}\n",
      "$$\n",
      "\n",
      "We already have $g^\\prime(h_k)$ so we can put it all together to give the update step for the second-layer weights\n",
      "\n",
      "$$w_{jk}\\leftarrow w_{jk} - \\eta \\frac{\\partial E}{\\partial w_{jk}}$$\n",
      "\n",
      "The missing piece is\n",
      "\n",
      "$$\\begin{aligned}\n",
      "\\frac{\\partial E}{\\partial w_{jk}} &= \\delta_0a_j \\cr\n",
      "&= (y_k - t_k)y_k(1-y_k)a_j\n",
      "\\end{aligned}$$\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "First layer weights"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Now we need the first layer weights - the hidden weights $v_{jk}$ \n",
      "* Remember we are going *back* propagation\n",
      "\n",
      "$$\\begin{aligned}\n",
      "\\delta_h &= \\sum_k \\frac{\\partial E}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_j} \\cr\n",
      "&= \\sum_k\\delta_0 \\frac{\\partial h_k}{\\partial h_j}\n",
      "\\end{aligned}$$\n",
      "\n",
      "* One thing to keep in mind, inputs to the output layer neurons come from the activation of the weighted average hidden layer neurons, using the second-layer weights\n",
      "\n",
      "$$h_k = g(\\sum_lw_{lk}h_l)$$\n",
      "\n",
      "and \n",
      "\n",
      "$$\\frac{\\partial h_k}{\\partial h_j}=\\frac{\\partial g(\\sum_l w_{lk}h_l)}{h_j}$$\n",
      "\n",
      "Noting that $\\frac{\\partial h_i}{\\partial h_j} = 0$ if $i\\neq l$ \n",
      "\n",
      "$$\\begin{aligned}\n",
      "\\frac{\\partial h_k}{\\partial h_j}&=w_{jk}g^{\\prime}(a_j) \\cr\n",
      "&=w_{jk}a_j(1-a_j)\n",
      "\\end{aligned}$$\n",
      "\n",
      "This gives a **delta term**\n",
      "\n",
      "$$\\delta_h = a_j(1-a_j)\\sum_k \\delta_0 w_{jk}$$\n",
      "\n",
      "So the update rule $v_{ij}\\leftarrow v_{ij} - \\eta\\frac{\\partial E}{\\partial v_{ij}}$ needs\n",
      "\n",
      "$$\\frac{\\partial E}{\\partial v_{ij}} = a_j(1-a_j)(\\sum_k \\delta_0w_{jk})x_i$$"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}